{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'C:/Users/1/attribution_project/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = 10\n",
    "winning_algorithm = 'CatBoostRegressor' #one of: 'RandomForestRegressor','LinearRegression','XGBRegressor', 'LGBMRegressor', 'CatBoostRegressor'\n",
    "winning_encoder = 'None' #one of: 'OneHotEncoder','CatBoostEncoder','None'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algorithms = [winning_algorithm]\n",
    "encoders = [winning_encoder]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining functions for interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "import matplotlib as plt\n",
    "import seaborn\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def d_intraMedianAll(df):\n",
    "    categories = list(df['category'].unique())\n",
    "    l_intra = []\n",
    "    for c in categories:\n",
    "        s = list(df.loc[df['category']==c, 'SHAP'])\n",
    "        l_intra.append(np.median(s))\n",
    "\n",
    "    return l_intra\n",
    "\n",
    "def d_intra(s):\n",
    "    c = list(itertools.combinations(s,2))\n",
    "    \n",
    "    m = []\n",
    "    for i in c:\n",
    "        m.append(abs(i[1]-i[0]))\n",
    "    if m == []:\n",
    "        m = [0]\n",
    "        \n",
    "    return(np.median(m))\n",
    "\n",
    "def d_intraAll(df):\n",
    "    categories = list(df['category'].unique())\n",
    "    l_intra = []\n",
    "    for c in categories:\n",
    "        s = list(df.loc[df['category']==c, 'SHAP'])\n",
    "        l_intra.append(d_intra(s))\n",
    "\n",
    "    return l_intra\n",
    "\n",
    "def d_inter(df):\n",
    "    categories = list(df['category'].unique())\n",
    "    \n",
    "    l_l_per_cat = []\n",
    "    for c in categories:\n",
    "        l_l_per_cat.append(list(df.loc[df['category']==c, 'SHAP']))\n",
    "    \n",
    "    l_l_pairs = list(itertools.combinations(l_l_per_cat,2))\n",
    "    \n",
    "    inter_meadians_per_cat = []\n",
    "    inter_means_per_cat = []\n",
    "    \n",
    "    for pair in l_l_pairs:\n",
    "        c = list(itertools.product(pair[0], pair[1]))\n",
    "        \n",
    "        m = []\n",
    "        for i in c:\n",
    "            m.append(abs(i[1]-i[0]))\n",
    "    \n",
    "        inter_meadians_per_cat.append(np.median(m))\n",
    "        inter_means_per_cat.append(np.mean(m))        \n",
    "    \n",
    "    return inter_meadians_per_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def global_SHAP(shap_v,df_v, metric='l_max_inter'):\n",
    "    \n",
    "    one_cat_vars = []\n",
    "    \n",
    "    l_intra_per_cat = []\n",
    "    l_inter_per_cat = []\n",
    "    l_max_intra = []\n",
    "    l_max_inter = []\n",
    "    l_inter_to_intra = []\n",
    "    l_intra_medians_per_cat = []\n",
    "    l_max_intra_median = []\n",
    "    \n",
    "    feature_list = df_v.columns\n",
    "    \n",
    "    for i in feature_list:\n",
    "        df_feature = pd.concat([df_v[i],shap_v[i]],axis=1)\n",
    "        df_feature.columns  = ['category','SHAP']\n",
    "        \n",
    "        #print(df_feature)\n",
    "        \n",
    "        intra_medians_per_cat = d_intraMedianAll(df_feature)\n",
    "        \n",
    "        intra_per_cat = d_intraAll(df_feature)\n",
    "        inter_per_cat = d_inter(df_feature)\n",
    "        \n",
    "        max_intra_median = np.max(intra_medians_per_cat )\n",
    "        max_intra = np.max(intra_per_cat)\n",
    "        \n",
    "        if len(inter_per_cat) > 0:\n",
    "            max_inter = np.max(inter_per_cat)\n",
    "            inter_to_intra = max_inter/max_intra\n",
    "        else:\n",
    "            print('one category variable:', i)\n",
    "            print('category:', df_feature['category'].unique())\n",
    "            one_cat_vars.append(i)\n",
    "            max_inter = np.NaN\n",
    "            inter_to_intra = np.NaN\n",
    "        \n",
    "        \n",
    "        l_intra_per_cat.append(intra_per_cat)\n",
    "        l_inter_per_cat.append(inter_per_cat)\n",
    "        l_max_intra.append(max_intra)\n",
    "        l_max_inter.append(max_inter)\n",
    "        l_inter_to_intra.append(inter_to_intra)\n",
    "        l_intra_medians_per_cat.append(intra_medians_per_cat) \n",
    "        l_max_intra_median.append(max_intra_median) \n",
    "        \n",
    "        \n",
    "    df_res = pd.concat([pd.Series(feature_list), pd.Series(l_intra_medians_per_cat), \n",
    "                        pd.Series(l_max_intra_median), pd.Series(l_intra_per_cat),\n",
    "                        pd.Series(l_inter_per_cat),pd.Series(l_max_intra),pd.Series(l_max_inter),\n",
    "                        pd.Series(l_inter_to_intra)],axis=1)#.fillna(0)\n",
    "\n",
    "    df_res.columns  = ['Variable', 'intra_medians_per_cat', 'l_max_intra_median', 'intra_per_cat',\n",
    "                       'l_inter_per_cat', 'l_max_intra', 'l_max_inter', 'inter_to_intra']\n",
    "    \n",
    "    df_res['Sign'] = np.where(df_res['inter_to_intra']<=1,'red','green')\n",
    "    \n",
    "    #drop one category variables\n",
    "    df_res = df_res[~df_res['Variable'].isin(one_cat_vars)]\n",
    "    \n",
    "    # Plot it\n",
    "    df_res = df_res.sort_values(by=metric, ascending = True)\n",
    "    colorlist = df_res['Sign']\n",
    "    \n",
    "    font = {'weight' : 'bold',\n",
    "            'size'   : 65}\n",
    "\n",
    "    plt.rc('font', **font)\n",
    "    \n",
    "    ax = df_res.plot.barh(x='Variable',y=metric,color = colorlist, figsize=(50,60),legend=False)\n",
    "    ax.set_xlabel(metric, fontsize=32)\n",
    "    ax.set_ylabel(\"Variable\", fontsize=75)\n",
    "    \n",
    "    return (df_res, one_cat_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SHAP_bars(df_shap,df, one_cat_vars, selected, abs_vals=False):\n",
    "    \n",
    "    shap_v = pd.DataFrame(df_shap)\n",
    "    feature_list = df.columns\n",
    "    shap_v.columns = feature_list\n",
    "    df_v = df.copy().reset_index().drop('index',axis=1)\n",
    "    \n",
    "    corr_df = pd.DataFrame(feature_list)\n",
    "    corr_df.columns  = ['Variable']\n",
    "    corr_df['Sign'] = np.where(corr_df['Variable'].isin(selected),'grey','blue')\n",
    "    corr_df['Sign'] = np.where(corr_df['Variable'].isin(one_cat_vars),'grey', corr_df['Sign'])\n",
    "    \n",
    "    # Plot it\n",
    "    if abs_vals:\n",
    "        shap_abs = np.abs(shap_v)\n",
    "    else:\n",
    "        shap_abs = shap_v\n",
    "        \n",
    "    font = {'weight' : 'bold',\n",
    "            'size'   : 65}\n",
    "        \n",
    "    k=pd.DataFrame(shap_abs.median()).reset_index()\n",
    "    k.columns = ['Variable','SHAP_abs']\n",
    "    k2 = k.merge(corr_df,left_on = 'Variable',right_on='Variable',how='inner')\n",
    "    k2 = k2.sort_values(by='SHAP_abs',ascending = True)\n",
    "    colorlist = k2['Sign']\n",
    "    ax = k2.plot.barh(x='Variable',y='SHAP_abs',color = colorlist, figsize=(50,60),legend=False)\n",
    "    if abs_vals:\n",
    "        ax.set_xlabel(\"Median absolute SHAP value\")\n",
    "    else:\n",
    "        ax.set_xlabel(\"Median SHAP Value\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_categories(feature, df_v, shap_v):\n",
    "    \n",
    "    df_feature = pd.concat([df_v[feature],shap_v[feature]],axis=1)\n",
    "    df_feature = pd.concat([df_v[feature],shap_v[feature]],axis=1)\n",
    "    df_feature.columns  = ['category','SHAP']\n",
    "    \n",
    "    b = seaborn.boxplot(data=df_feature, x='category', y='SHAP')\n",
    "    b.set_xlabel(feature)\n",
    "    b.set_ylabel(\"SHAP value\")\n",
    "    b.tick_params()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_category(feature,category,selected,expected_value,df_v,shap_v):\n",
    "    df_v = df_v[df_v[feature]==category]\n",
    "    i1 = df_v.index\n",
    "    \n",
    "    i2 = shap_v.index\n",
    "    shap_v = shap_v[i2.isin(i1)]\n",
    "\n",
    "    selected.append(feature)\n",
    "    \n",
    "    expected_value += np.mean(shap_v[feature])\n",
    "\n",
    "    df_v = df_v.drop(feature, axis=1).reset_index(drop=True)\n",
    "    shap_v = shap_v.drop(feature, axis=1).reset_index(drop=True)\n",
    "\n",
    "    return (selected, expected_value, df_v,shap_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shap(algorithm, encoder, path, dataset='test', shapley_library='shap'):\n",
    "    \n",
    "    import pickle\n",
    "    \n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    \n",
    "    import shap\n",
    "    if shapley_library == 'shap':\n",
    "        pass\n",
    "    elif shapley_library == 'acv':\n",
    "        from acv_explainers import ACVTree\n",
    "    \n",
    "    path_in = path + 'output/' + algorithm + '_' + encoder + '_'\n",
    "    path_out = path + 'output/' + library +'/'+ dataset + '/' + algorithm + '_' + encoder + '_'\n",
    "    path_data = path + 'data/'\n",
    "    \n",
    "    #load data\n",
    "    model = pickle.load(open(path_in + 'model.pkl','rb'))\n",
    "\n",
    "    X = pickle.load(open(path_in + 'X_' + dataset + '.pkl','rb'))\n",
    "    y = pickle.load(open(path_data + 'y_' + dataset + '.pkl','rb'))\n",
    "    y_pred = pickle.load(open(path_in + 'y_' + dataset + '_pred.pkl','rb'))\n",
    "    \n",
    "    if encoder not in encoders_1to1:\n",
    "        enc_list_col_ids = pickle.load(open(path_in + 'enc_list_col_ids.pkl','rb'))\n",
    "        enc = pickle.load(open(path_in + 'enc.pkl','rb'))\n",
    "        \n",
    "    X_orig = pickle.load(open(path_data + 'X_' + dataset + '.pkl','rb'))\n",
    "    \n",
    "    #reset indexes\n",
    "    X=X.reset_index(drop=True)\n",
    "    y=y.reset_index(drop=True)\n",
    "    X_orig=X_orig.reset_index(drop=True)\n",
    "    \n",
    "    #X_display - from orig, but integers repalicn categories\n",
    "    X_display = X_orig.copy()\n",
    "    \n",
    "    #all dependent variables are categorical\n",
    "    categorical_vars = X_orig.columns\n",
    "\n",
    "    categories = {}\n",
    "    encoding = {}\n",
    "    decoding = {}\n",
    "\n",
    "    for var in categorical_vars:\n",
    "        categories[var] = list(X_display[var].unique())\n",
    "        encoding[var] = {k: v for v, k in enumerate(categories[var])}\n",
    "        decoding[var] = {v: k for k, v in encoding[var].items()}\n",
    "    \n",
    "        X_display[var] = X_display[var].map(encoding[var]).astype('int')\n",
    "    \n",
    "    if shapley_library=='shap':\n",
    "        #fit the SHAP model\n",
    "        shap.initjs()\n",
    "        if algorithm == 'LinearRegression':\n",
    "            X_train = pickle.load(open(path_in + 'X_train.pkl','rb'))\n",
    "            explainer = shap.LinearExplainer(model, X_train)\n",
    "        else:\n",
    "            explainer = shap.TreeExplainer(model)\n",
    "        shap_values = explainer.shap_values(X)\n",
    "    \n",
    "        #transforming for the SHAP model\n",
    "        shap_values_df = pd.DataFrame(shap_values, columns=X.columns, index=X.index)\n",
    "\n",
    "    \n",
    "        if encoder not in encoders_1to1:\n",
    "            for mapping in enc.mapping:\n",
    "                shap_values_df[mapping['col']]=shap_values_df.loc[:, mapping['mapping'].columns].sum(axis=1)\n",
    "                shap_values_df.drop(mapping['mapping'].columns, axis=1, inplace=True)\n",
    "    \n",
    "    if shapley_library=='acv':\n",
    "        #fit the ACV model\n",
    "        X_train = pickle.load(open(path_in + 'X_train.pkl','rb'))\n",
    "        \n",
    "        acvtree = ACVTree(model=model, data=X_train.values, cache_normalized=True)\n",
    "\n",
    "        if encoder in encoders_1to1:\n",
    "            shap_values_acv = acvtree.py_shap_values(X.values)\n",
    "        else:\n",
    "            shap_values_acv = acvtree.shap_values(X.values, C=enc_list_col_ids)\n",
    "    \n",
    "        #transforming for the ACV model\n",
    "        shap_values_df = pd.DataFrame(shap_values_acv[:, :, 0], columns=X.columns, index=X.index)\n",
    "            \n",
    "        if encoder not in encoders_1to1:\n",
    "            for mapping in enc.mapping:\n",
    "                shap_values_df[mapping['col']] =  shap_values_df.loc[:, mapping['mapping'].columns[0]]\n",
    "                shap_values_df.drop(mapping['mapping'].columns, axis=1, inplace=True)\n",
    "    \n",
    "    \n",
    "    #analysis\n",
    "    if shapley_library == 'shap':\n",
    "        expected_value = explainer.expected_value\n",
    "        print(expected_value)\n",
    "    \n",
    "        #out of the box graphs\n",
    "        plt.clf()\n",
    "        shap.decision_plot(explainer.expected_value, shap_values_df.to_numpy(), X_orig, \n",
    "                           feature_names=list(shap_values_df.columns), show=False)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(path_out + 'shap1.jpg')\n",
    "    \n",
    "        plt.clf()\n",
    "        shap.decision_plot(explainer.expected_value, explainer.shap_values(X), X, \n",
    "                           feature_names=list(X.columns), show=False)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(path_out + 'shap1a.jpg')\n",
    "    \n",
    "    plt.clf()\n",
    "    shap.summary_plot(shap_values_df.to_numpy(), features=X_display, feature_names=X_display.columns,\n",
    "                      max_display=X_display.shape[1], show=False)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(path_out + 'shap2.jpg')\n",
    "    \n",
    "    plt.clf()\n",
    "    shap.summary_plot(shap_values_df.to_numpy(), features=X_display, feature_names=X_display.columns,\n",
    "                      plot_type='bar', max_display=X_display.shape[1], show=False)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(path_out + 'shap3.jpg')\n",
    "    \n",
    "    #custom graphs\n",
    "    \n",
    "    df_shap = shap_values_df\n",
    "    df = X_orig\n",
    "    \n",
    "    selected = []\n",
    "\n",
    "    # Make a copy of the input data\n",
    "    shap_v = pd.DataFrame(df_shap)\n",
    "    feature_list = df.columns\n",
    "    shap_v.columns = feature_list\n",
    "    df_v = df.copy().reset_index().drop('index',axis=1)\n",
    "    \n",
    "    #custom graphs\n",
    "    plt.clf()\n",
    "    df_res, one_cat_vars = global_SHAP(shap_v,df_v, metric='inter_to_intra' )\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(path_out + 'shap4.jpg')\n",
    "    \n",
    "    plt.clf()\n",
    "    df_res_final, one_cat_vars = global_SHAP(shap_v,df_v, metric='l_max_inter')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(path_out + 'shap5.jpg')\n",
    "    \n",
    "    plt.clf()\n",
    "    SHAP_bars(shap_v,df_v, one_cat_vars, selected, abs_vals=True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(path_out + 'shap6.jpg')\n",
    "    \n",
    "    plt.clf()\n",
    "    SHAP_bars(shap_v,df_v, one_cat_vars, selected, abs_vals=False)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(path_out + 'shap7.jpg')\n",
    "    \n",
    "    plt.clf()\n",
    "    df_res, one_cat_vars = global_SHAP(shap_v,df_v, metric='l_max_intra_median')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(path_out + 'shap8.jpg')\n",
    "    \n",
    "    #global importance and separability\n",
    "    \n",
    "    df_importance = df_res_final[['Variable','l_max_inter', 'inter_to_intra']]\n",
    "    s=df_importance['l_max_inter'].sum()\n",
    "    df_importance['l_max_inter_norm']=df_importance['l_max_inter']/s\n",
    "\n",
    "    df_importance = df_importance.sort_values(by='l_max_inter', ascending=False).reset_index(drop=True)\n",
    "    \n",
    "    #plotting individual variables: categories per variable\n",
    "    ordered_variables = list(df_importance['Variable'])\n",
    "    \n",
    "    img_ind = 90 \n",
    "    for variable in ordered_variables:\n",
    "        plt.clf()\n",
    "        show_categories(variable, df_v, shap_v)\n",
    "        #plt.tight_layout()\n",
    "        plt.savefig(path_out + 'shap' + str(img_ind) + '.jpg')\n",
    "        \n",
    "        img_ind += 1\n",
    "        \n",
    "    #save pickle and csv\n",
    "    pickle.dump(df_importance, open(path_out + 'importance.pkl','wb'))\n",
    "                \n",
    "    df_importance.columns = df_importance.columns.str.replace('_',' ')\n",
    "    df_importance['Variable'] = df_importance['Variable'].str.replace('_',' ')\n",
    "    df_importance.round(3).to_csv(path_out + 'importance.csv', index=False)\n",
    "    \n",
    "    return(df_importance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoders_1to1 = ['None','CatBoostEncoder'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_in = path + 'results/m' + str(m) + '/'\n",
    "\n",
    "for library in ['acv', 'shap']:\n",
    "    for dataset in ['train', 'test']:\n",
    "\n",
    "        df_results = pd.DataFrame()\n",
    "\n",
    "        for algorithm in algorithms:\n",
    "            for encoder in encoders:\n",
    "\n",
    "                if not((algorithm in ('LinearRegression','CatBoostRegressor') and library=='acv') \n",
    "                       or (encoder=='None' and algorithm!= 'CatBoostRegressor')):\n",
    "\n",
    "                    print(library, algorithm, encoder, dataset)\n",
    "\n",
    "                    df_importance = shap(algorithm, encoder, path=path_in, dataset=dataset, \n",
    "                                         shapley_library=library)\n",
    "                    df_importance['algorithm'] = algorithm\n",
    "                    df_importance['encoder'] = encoder\n",
    "\n",
    "                    df_results = df_results.append(df_importance)\n",
    "\n",
    "\n",
    "        path_out_global = path_in + 'output/' + library +'/'+ dataset + '/'\n",
    "\n",
    "        pickle.dump(df_results, open(path_out_global + 'df_importance_all.pkl','wb'))\n",
    "\n",
    "        df_results.columns = df_results.columns.str.replace('_',' ')\n",
    "        df_results.round(3).to_csv(path_out_global + 'importance_all.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
